# N8N Integration Docker Compose Override
# This file extends the base LibreChat docker-compose.yml with N8N services
# Usage: docker-compose -f docker-compose.yml -f docker-compose.n8n.yml up

networks:
  n8n_network:
    external: false

volumes:
  n8n_storage:
  postgres_n8n_storage:
  ollama_storage:
  qdrant_storage:

x-n8n: &service-n8n
  image: n8nio/n8n:latest
  networks: ['default', 'n8n_network']
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=postgres_n8n
    - DB_POSTGRESDB_USER=${N8N_POSTGRES_USER:-n8n}
    - DB_POSTGRESDB_PASSWORD=${N8N_POSTGRES_PASSWORD:-n8npassword}
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:-myEncryptionKey}
    - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_JWT_SECRET:-myJwtSecret}
    - OLLAMA_HOST=ollama:11434
    - N8N_SECURE_COOKIE=false
    - N8N_SAMESITE_COOKIE=none
    - N8N_IFRAME_DENY=false
    - N8N_CORS_ORIGIN=*
    - N8N_IFRAME_ALLOWED_ORIGINS=http://138.199.157.172:3090,http://localhost:3090,http://138.199.157.172:3080,http://localhost:3080

x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: n8n-ollama
  networks: ['n8n_network']
  restart: unless-stopped
  ports:
    - "11434:11434"
  volumes:
    - ollama_storage:/root/.ollama

services:
  # Extend the API service to expose N8N URL as environment variable
  api:
    environment:
      - N8N_URL=http://n8n-proxy:8080
      - REACT_APP_N8N_URL=http://138.199.157.172:8080

  # PostgreSQL for N8N
  postgres_n8n:
    image: postgres:16-alpine
    hostname: postgres_n8n
    container_name: n8n-postgres
    networks: ['n8n_network']
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${N8N_POSTGRES_USER:-n8n}
      - POSTGRES_PASSWORD=${N8N_POSTGRES_PASSWORD:-n8npassword}
      - POSTGRES_DB=${N8N_POSTGRES_DB:-n8n}
    volumes:
      - postgres_n8n_storage:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${N8N_POSTGRES_USER:-n8n} -d ${N8N_POSTGRES_DB:-n8n}']
      interval: 5s
      timeout: 5s
      retries: 10

  # N8N Import service
  n8n-import:
    <<: *service-n8n
    hostname: n8n-import
    container_name: n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - "n8n import:credentials --separate --input=/demo-data/credentials && n8n import:workflow --separate --input=/demo-data/workflows"
    volumes:
      - ./n8n/demo-data:/demo-data
    depends_on:
      postgres_n8n:
        condition: service_healthy

  # Main N8N service
  n8n:
    <<: *service-n8n
    hostname: n8n
    container_name: n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_WEBHOOK_URL=http://138.199.157.172:8080/
      - N8N_SECURE_COOKIE=false
      - N8N_SAMESITE_COOKIE=none
      - N8N_SECURITY_AUDIT_DAYS=0
      - N8N_DISABLE_UI=false
      - N8N_IFRAME_DENY=false
      - N8N_CORS_ORIGIN=*
      - N8N_IFRAME_ALLOWED_ORIGINS=http://138.199.157.172:3090,http://localhost:3090,http://138.199.157.172:3080,http://localhost:3080
      - RESPONSE_MODE=RESPONSE_NEVER_SET_COOKIES
      - N8N_ENDPOINT_REST=rest
      - N8N_ENDPOINT_WEBHOOK=webhook
      - N8N_METRICS=false
    volumes:
      - n8n_storage:/home/node/.n8n
      - ./n8n/demo-data:/demo-data
      - ./shared:/data/shared
    depends_on:
      postgres_n8n:
        condition: service_healthy
      n8n-import:
        condition: service_completed_successfully

  # Nginx proxy for n8n to handle iframe headers
  n8n-proxy:
    image: nginx:alpine
    container_name: n8n-proxy
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./n8n/nginx.conf:/etc/nginx/nginx.conf:ro
    networks: ['default', 'n8n_network']
    depends_on:
      - n8n

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant
    hostname: qdrant
    container_name: n8n-qdrant
    networks: ['n8n_network']
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  # Ollama for local LLM support
  ollama:
    <<: *service-ollama
    profiles: ["n8n-cpu"]

  # Ollama GPU version
  ollama-gpu:
    <<: *service-ollama
    profiles: ["n8n-gpu-nvidia"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Initialize Ollama with Mistral model
  ollama-init:
    image: ollama/ollama:latest
    networks: ['n8n_network']
    container_name: ollama-pull-mistral
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=ollama:11434
    command:
      - "-c"  
      - "sleep 10; ollama pull mistral:latest"
    profiles: ["n8n-cpu"]
    depends_on:
      - ollama